data
write.csv(data,file="F:/LPI/data1.csv")
#calculate dissimilarities, use function "vegdist"in VEGAN package
data.dis<-vegdist(data)
data.dis
dis.matrix<-as.matrix(data.dis)
dis.matrix
write.csv(dis.matrix,file="F:/LPI/LPIDistanceMatrix.csv")
#calculate isoMDS function, use isoDS function in MASS package
data.mds<-isoMDS(data.dis)
data.mds
#--------stressplot and ordiplot----------
stressplot(data.mds,data.dis)
stressplot(data.mds,data.dis)
stressplot(data.mds,data.dis)
library(vegan)
library(MASS)
#---NMDS Script for Plant Community Ecology--------
#### Took the file F:/LPI/LPIRelativeCover.csv and used
#### SUM and COUNTIF in Excel to find the colum sums and
#### removed those with COUNTIF below 5 AND SUMS below .05 (had to both be below to be removed)
#### number of sites each veg was found at then I manually
#### Also cleaned up the data (combined SALS0 and SALSO, etc...)
data<-read.csv("F:/LPI/LPIRelativeCoverCommonInExcel.csv",header=TRUE,row.names=1)
data
write.csv(data,file="F:/LPI/data1.csv")
###remove empty columns
#lpi.full<-as.matrix(data1[-which(colSums(data1)==0),])
#lpi.full
#remove negative values
#data1[42,63]<-0
#data1[62,41]<-0
#data1
#calculate dissimilarities, use function "vegdist"in VEGAN package
data.dis<-vegdist(data)
data.dis
dis.matrix<-as.matrix(data.dis)
dis.matrix
write.csv(dis.matrix,file="F:/LPI/LPIDistanceMatrix.csv")
#calculate isoMDS function, use isoDS function in MASS package
data.mds<-isoMDS(data.dis)
data.mds
#--------stressplot and ordiplot----------
stressplot(data.mds,data.dis)
ordiplot(data.mds,type="t")
par(mar)
par("mar")
par(mar=c(1,1,1,1))
stressplot(data.mds,data.dis)
ordiplot(data.mds,type="t")
par(mar=c(1,1,1,1))
par("mar")
stressplot(data.mds,data.dis)
par("mar")
par(mar=c(5.1, 4.1, 4.1, 2.1))
par("mar")
stressplot(data.mds,data.dis)
par(mar=c(5, 4, 4, 2))
par("mar")
stressplot(data.mds,data.dis)
par(mar=c(2, 2, 2, 2))
par("mar")
stressplot(data.mds,data.dis)
ordiplot(data.mds,type="t")
data.mds<-isoMDS(data.dis)
data.mds
ordiplot(data.mds,type="t")
stressplot(data.mds,data.dis)
par(mar=c(5, 5, 5, 5))
par("mar")
stressplot(data.mds,data.dis)
par(mar=c(10, 10, 10, 10))
par("mar")
stressplot(data.mds,data.dis)
dev.off()
stressplot(data.mds,data.dis)
library(vegan)
library(MASS)
#---NMDS Script for Plant Community Ecology--------
#### Took the file F:/LPI/LPIRelativeCover.csv and used
#### SUM and COUNTIF in Excel to find the colum sums and
#### number of sites each veg was found at then I manually
#### removed those with COUNTIF below 5 AND SUMS below .05 (had to both be below to be removed)
#### Also cleaned up the data (combined SALS0 and SALSO, etc...)
data<-read.csv("F:/LPI/LPIRelativeCoverCommonInExcel.csv",header=TRUE,row.names=1)
data
write.csv(data,file="F:/LPI/data1.csv")
###remove empty columns
#lpi.full<-as.matrix(data1[-which(colSums(data1)==0),])
#lpi.full
#remove negative values
#data1[42,63]<-0
#data1[62,41]<-0
#data1
#calculate dissimilarities, use function "vegdist"in VEGAN package
data.dis<-vegdist(data)
data.dis
dis.matrix<-as.matrix(data.dis)
dis.matrix
write.csv(dis.matrix,file="F:/LPI/LPIDistanceMatrix.csv")
#calculate isoMDS function, use isoDS function in MASS package
data.mds<-isoMDS(data.dis)
data.mds
#--------stressplot and ordiplot----------
stressplot(data.mds,data.dis)
ordiplot(data.mds,type="t")
data.mds<-isoMDS(data.dis)
data.mds
stressplot(data.mds,data.dis)
data.mds<-isoMDS(data.dis)
data.mds
data.dis<-vegdist(data)
data.dis
dis.matrix<-as.matrix(data.dis)
dis.matrix
write.csv(dis.matrix,file="F:/LPI/LPIDistanceMatrix.csv")
data.mds<-isoMDS(data.dis)
data.mds
?isoMDS
vegdist
?Vegdist
?vegdist
data.dis<-vegdist(data,method="bray")
data.dis
dis.matrix<-as.matrix(data.dis)
dis.matrix
write.csv(dis.matrix,file="F:/LPI/LPIDistanceMatrix.csv")
#calculate isoMDS function, use isoDS function in MASS package
data.mds<-isoMDS(data.dis)
data.mds
#--------stressplot and ordiplot----------
stressplot(data.mds,data.dis)
ordiplot(data.mds,type="t")
data.env<-read.csv("F:/Soils/SoilEnvironmentalData.csv",header=TRUE,row.names=1)
data.env<-data.env[,c(5:8,11,15:18)]
data.env<-data.env[,c(1:4,8,9)]
data.mds<-metaMDS(comm=data,distance="bray",trace=FALSE)
data.mds
data.mds<-metaMDS(comm=data,distance="euclidian",trace=FALSE)
data.mds
data.mds<-metaMDS(comm=data,distance="jaccard",trace=FALSE)
data.mds
data.mds<-metaMDS(comm=data,distance="euclidian",trace=FALSE)
data.mds
fit<-envfit(data.mds,data.env,perm=1000)
fit
plot(data.mds,type="t",main="NMDS using Euclidean Distance")
#plot environmental loadings
plot(fit)
library(vegan)
library(MASS)
#---NMDS Script for Plant Community Ecology--------
data<-read.csv("F:/Rcode/KLEEdataE.csv",header=TRUE)
data<-read.csv("F:/RcodeWILD/KLEEdataE.csv",header=TRUE)
data
data1<-data[,3:ncol(data)]
data1
#calculate dissimilarities, use function "vegdist"in VEGAN package
data.dis<-vegdist(data1)
data.dis
#calculate isoMDS function, use isoDS function in MASS package
data.mds<-isoMDS(data.dis)
#--------stressplot and ordiplot----------
stressplot(data.mds,data.dis)
ordiplot(data.mds,type="t")
#read in environmental data
data.env<-data[,1:2]
data.env
#set distclass to factor
data.env[,1]=as.factor(data.env[,1])
data.env[,1]
is.factor(data.env[,1])
#set distclass to factor
data.env[,2]=as.factor(data.env[,2])
data.env[,2]
is.factor(data.env[,2])
data.mds<-metaMDS(comm=data1,distance="euclidean",trace=FALSE)
data.mds
#function "envfit" fits environmental vectors or factors onto an ordination.
#requires ordination plot first before plot(fit)
fit<-envfit(data.mds,data.env,perm=1000)
fit
#plotMDS
plot(data.mds,type="t",main="NMDS using Euclidean Distance")
#plot environmental loadings
plot(fit)
setwd("D:/BeefBasin")
library(rgdal)
library(ggmap)
library(ggplot2)
library(aqp)
library(plyr)
library(reshape2)
#1. USGS (cLHS) pedon data
{
#1.a Horizon data. Remove useless columns and make names the same as April's data
usgsH <- read.csv("./PedonData/formattedR/allPedons.csv", header = TRUE)
#1. USGS (cLHS) pedon data
{
#1.a Horizon data. Remove useless columns and make names the same as April's data
usgsH <- read.csv("./PedonData/formattedR/allPedons.csv", header = TRUE)
usgsH1 <- usgsH[,-c(2,3)]
names(usgsH1)[c(15,17)] <- c('SandPercent', 'ClayPercent')
usgsH2 <- usgsH1[,-c(11,13)] #Remove RF_knd, RF_sz (not needed)
usgsH2 <- usgsH2[c(1:10,12:17,11)] #reshuffle column names for easy joining with April's data
#1.b Rosetta data, give meaningful names, calculate Available Water Holding Capacity (AWC)
usgsR <- read.csv("./PedonData/Rosetta/cLHS_Rosetta_AWC.csv", header = TRUE)
usgsR[,3] <- round(usgsR[,3], 3) #round to 3 decimal places
#Join horizon data and AWC
usgsH3 <- cbind(usgsH2, usgsR$AWHC)
names(usgsH3)[18] <- 'AWC'
#1.c Site data. Keep CarbonateState, ExcavationDepth, IRLDepth, IRL150
usgsS <- read.csv("./PedonData/formattedR/Site_Data.csv", header = TRUE)
usgsS1 <- usgsS[,c(11,10,7:9)]
#1.d Location data
usgsL <- read.csv("./PedonData/formattedR/locInfo.csv", header = TRUE)
names(usgsL) [1:3] <- c('PedonID', 'Latitude', 'Longitude') #Make names consistent with April's data
usgsL <- usgsL[ , -which(names(usgsL) %in% 'Date')] #Remove date field
}
#1.a Horizon data. Remove useless columns and make names the same as April's data
usgsH <- read.csv("./PedonData/formattedR/allPedons.csv", header = TRUE)
setwd("F:/BeefBasin Data for April/BeefBasin Data for April")
setwd("F:/BeefBasin Data for April/BeefBasin Data for April")
library(rgdal)
library(ggmap)
library(ggplot2)
library(aqp)
library(plyr)
library(reshape2)
#1. USGS (cLHS) pedon data
#1.a Horizon data. Remove useless columns and make names the same as April's data
usgsH <- read.csv("./PedonData/formattedR/allPedons.csv", header = TRUE)
setwd("F:/BeefBasin")
setwd("F:/BeefBasin Data for April/BeefBasin Data for April/formattedR")
setwd("F:/BeefBasin Data for April")
setwd("F:/BeefBasin Data for April/BeefBasin")
setwd("F:/BeefBasin")
setwd("F:/BeefBasin Data for April/BeefBasin")
setwd("F:/BeefBasin")
setwd("F:/BeefBasin")
setwd("F:/BeefBasin Data for April/BeefBasin")
setwd("F:/BeefBasin Data for April/BeefBasin")
library(rgdal)
library(ggmap)
library(ggplot2)
library(aqp)
library(plyr)
library(reshape2)
#1. USGS (cLHS) pedon data
#1.a Horizon data. Remove useless columns and make names the same as April's data
usgsH <- read.csv("./PedonData/formattedR/allPedons.csv", header = TRUE)
usgsH1 <- usgsH[,-c(2,3)]
usgsH <- read.csv("./formattedR/allPedons.csv", header = TRUE)
usgsH1 <- usgsH[,-c(2,3)]
names(usgsH1)[c(15,17)] <- c('SandPercent', 'ClayPercent')
usgsH2 <- usgsH1[,-c(11,13)] #Remove RF_knd, RF_sz (not needed)
usgsH <- read.csv("./formattedR/allPedons.csv", header = TRUE)
usgsH1 <- usgsH[,-c(2,3)]
names(usgsH1)[c(15,17)] <- c('SandPercent', 'ClayPercent')
usgsH2 <- usgsH1[,-c(11,13)] #Remove RF_knd, RF_sz (not needed)
usgsH2 <- usgsH2[c(1:10,12:17,11)] #reshuffle column names for easy joining with April's data
#1.b Rosetta data, give meaningful names, calculate Available Water Holding Capacity (AWC)
usgsR <- read.csv("./Rosetta/cLHS_Rosetta_AWC.csv", header = TRUE)
usgsR[,3] <- round(usgsR[,3], 3) #round to 3 decimal places
#Join horizon data and AWC
usgsH3 <- cbind(usgsH2, usgsR$AWHC)
names(usgsH3)[18] <- 'AWC'
#1.c Site data. Keep CarbonateState, ExcavationDepth, IRLDepth, IRL150
usgsS <- read.csv("./formattedR/Site_Data.csv", header = TRUE)
usgsS1 <- usgsS[,c(11,10,7:9)]
#1.d Location data
usgsL <- read.csv("./formattedR/locInfo.csv", header = TRUE)
names(usgsL) [1:3] <- c('PedonID', 'Latitude', 'Longitude') #Make names consistent with April's data
usgsL <- usgsL[ , -which(names(usgsL) %in% 'Date')] #Remove date field
setwd("C:/Users/Grant/Documents/Beef Basin 2013/BeefBasin Data for April/BeefBasin")
library(rgdal)
library(ggmap)
library(ggplot2)
library(aqp)
library(plyr)
library(reshape2)
#1. USGS (cLHS) pedon data
#1.a Horizon data. Remove useless columns and make names the same as April's data
usgsH <- read.csv("./formattedR/allPedons.csv", header = TRUE)
usgsH1 <- usgsH[,-c(2,3)]
names(usgsH1)[c(15,17)] <- c('SandPercent', 'ClayPercent')
usgsH2 <- usgsH1[,-c(11,13)] #Remove RF_knd, RF_sz (not needed)
usgsH2 <- usgsH2[c(1:10,12:17,11)] #reshuffle column names for easy joining with April's data
#1.b Rosetta data, give meaningful names, calculate Available Water Holding Capacity (AWC)
usgsR <- read.csv("./Rosetta/cLHS_Rosetta_AWC.csv", header = TRUE)
usgsR[,3] <- round(usgsR[,3], 3) #round to 3 decimal places
#Join horizon data and AWC
usgsH3 <- cbind(usgsH2, usgsR$AWHC)
names(usgsH3)[18] <- 'AWC'
#1.c Site data. Keep CarbonateState, ExcavationDepth, IRLDepth, IRL150
usgsS <- read.csv("./formattedR/Site_Data.csv", header = TRUE)
usgsS1 <- usgsS[,c(11,10,7:9)]
#1.d Location data
usgsL <- read.csv("./formattedR/locInfo.csv", header = TRUE)
names(usgsL) [1:3] <- c('PedonID', 'Latitude', 'Longitude') #Make names consistent with April's data
usgsL <- usgsL[ , -which(names(usgsL) %in% 'Date')] #Remove date field
#2. April pedon data
{
#2.a horizon data
aH <- read.csv("./SoilData_cwb.csv", header = TRUE)
#2.a horizon data
aH <- read.csv("./SoilData_cwb.csv", header = TRUE)
aH1 <- aH[,c(1:21)]#remove site data
names(aH1)[c(1,21)] <- c('PedonID', 'Effer') #make names match usgs data
#April entered rock fragment percentages by size class. To get total RF sum over all size classes
aH1$RF_perc <- apply(aH1[,c(12:16)], 1, FUN = sum, na.rm = T)
aH2 <- aH1[,-c(12:16)]
#2.b Rosetta data, give meaningful names, calculate Available Water Holding Capacity (AWC)
aR <- read.csv("./Rosetta/April_Rosetta_AWC.csv", header = TRUE)
aR[,3] <- round(aR[,3], 3) #round to 3 decimal places
#Join horizon data and AWC
aH3 <- cbind(aH2, aR$AWHC)
names(aH3)[c(2,3,18)] <- c('Top','Bottom','AWC')
#2.b April site data
aS <- read.csv("./SiteData.csv", header = TRUE)
aS1 <- aS[,c(1,12,14:16)] #Keep CarbonateState, ExcavationDepth, IRLDepth, IRL150
names(aS1)[1] <- 'PedonID'
#2.c April Location data
aL <-read.csv("./locInfo.csv", header = TRUE)
names(aL)[c(1,4,5)] <- c('PedonID','Northing', 'Easting')
}
#3. Vegetation data
Cveg <- read.csv("./VegetationDataCompiled_edit.csv")
Cveg <- Cveg[,-1] #Remove first column, unneeded
#4. Combine datasets
#4.a Horizon data - total 176 pedons.
cHori <- rbind(usgsH3,aH3) #Combined USGS and April's pedons - need to export these as KML for coolness.
#In order to investigate relationships between pedon data and veg, I must have one value per variable per pedon location.
#Max Clay
MaxClay <- ddply(cHori, 'PedonID', summarize, MaxClay = max(ClayPercent, na.rm = T))
#Max Rock fragment content
MaxRF <- ddply(cHori, 'PedonID', summarize, MaxRF = max(RF_perc, na.rm = T))
#AWC
TotalAWC <- ddply(cHori, 'PedonID', summarize, TotalAWC = sum(AWC, na.rm = T))
MaxAWC <- ddply(cHori, 'PedonID', summarize, MaxAWC = max(AWC, na.rm = T))
#Now calculate depth weighted averages of each continuous variable, then append these to the other variables.
#Convert to SoilProfileCollection
depths(cHori) <- PedonID ~ Top + Bottom
# within each profile, compute weighted means, over the intervals: 0-25,0-50,0-100, removing NA if present
d25 <- slab(cHori, PedonID ~ AWC, slab.structure = c(0,25), slab.fun = mean, na.rm=TRUE)
d50 <- slab(cHori, PedonID ~ AWC, slab.structure = c(0,50), slab.fun = mean, na.rm=TRUE)
d100 <- slab(cHori, PedonID ~ AWC, slab.structure = c(0,100), slab.fun = mean, na.rm=TRUE)
# reshape to wide format, remove unneeded variables and rename.
AWC25 <- dcast(d25, PedonID + top + bottom ~ variable, value.var = 'value')
AWC25 <- AWC25[,-c(2,3)]
names(AWC25)[2] <- 'AWC25'
AWC50 <- dcast(d50, PedonID + top + bottom ~ variable, value.var = 'value')
AWC50 <- AWC50[,-c(2,3)]
names(AWC50)[2] <- 'AWC50'
AWC100 <- dcast(d100, PedonID + top + bottom ~ variable, value.var = 'value')
AWC100 <- AWC100[,-c(2,3)]
names(AWC100)[2] <- 'AWC100'
#4.b Site data
cSite <- rbind(usgsS1, aS1) #Combined USGS and April's site data
#Change IRLDepth 150+ to 150 for plotting purposes (makes everything numeric).
cSite$IRLDepth <- ifelse(cSite$IRLDepth == '150+',150,as.numeric(as.character(cSite$IRLDepth)))
#Was there an infiltration restricting layer within 100 or 50 cm? Some of these are NA because my IRLDepth was NA if the excavation depth did not reach 150cm. However, if excavation depth was > 100 or 50 cm, then I can change NA values to 'N' - I would have to do this mannually.
cSite$IRL100 <- ifelse(cSite$IRLDepth >  100, 'N', 'Y')
cSite$IRL50 <- ifelse(cSite$IRLDepth >  50, 'N', 'Y')
{ cLoc <- rbind(usgsL, aL) #Combined USGS and April's GPS location info
s1 <- join(cLoc, cSite, by = 'PedonID', type = 'inner')
s2 <- join(s1, MaxClay, by = 'PedonID', type = 'inner')
s3 <- join(s2, MaxRF, by = 'PedonID', type = 'inner')
s4 <- join(s3, MaxAWC, by = 'PedonID', type = 'inner')
s5 <- join(s4, TotalAWC, by = 'PedonID', type = 'inner')
s6 <- join(s5, AWC25, by = 'PedonID', type = 'inner')
s7 <- join(s6, AWC50, by = 'PedonID', type = 'inner')
s8 <- join(s7, AWC100, by = 'PedonID', type = 'inner')
s1 <- join(cLoc, cSite, by = 'PedonID', type = 'inner')
s2 <- join(s1, MaxClay, by = 'PedonID', type = 'inner')
s3 <- join(s2, MaxRF, by = 'PedonID', type = 'inner')
{ cLoc <- rbind(usgsL, aL) #Combined USGS and April's GPS location info
{ cLoc <- rbind(usgsL, aL) #Combined USGS and April's GPS location info
#Plot boundary and points on a Google map, because it's cool.
#study area boundary
#BB_bound <- readOGR("./Spatial_Data/StudyArea", "NCSS_StudyArea5Feb2013")
#BB_bound2 <- spTransform(BB_bound, CRS("+proj=longlat +datum=WGS84")) #Convert from UTM to lat/long for plotting.
#dfBound <- fortify(BB_bound2) #Convert to dataframe for plotting
#Get Google map
# BBmap <- get_map(location = c(lon = -109.9, lat = 37.99), zoom = 12,maptype = "terrain", scale = 2)
#Plot polygon and points on map
# ggmap(BBmap) +
# geom_polygon(aes(x=long, y=lat, group=group), data = dfBound, fill = NA, colour = 'black', size = 0.75) +
# geom_point(aes(x = Longitude, y = Latitude), data = cLoc, alpha = 0.7)
}
s1 <- join(cLoc, cSite, by = 'PedonID', type = 'inner')
s2 <- join(s1, MaxClay, by = 'PedonID', type = 'inner')
s3 <- join(s2, MaxRF, by = 'PedonID', type = 'inner')
s4 <- join(s3, MaxAWC, by = 'PedonID', type = 'inner')
s5 <- join(s4, TotalAWC, by = 'PedonID', type = 'inner')
s6 <- join(s5, AWC25, by = 'PedonID', type = 'inner')
s7 <- join(s6, AWC50, by = 'PedonID', type = 'inner')
s8 <- join(s7, AWC100, by = 'PedonID', type = 'inner')
SV <- join(s8, Cveg, by = 'PedonID', type = 'inner')
#Make sure that categorical data is a factor.
SV$CarbonateStage <- as.factor(SV$CarbonateStage)
SV$IRL100 <- as.factor(SV$IRL100)
SV$IRL50 <- as.factor(SV$IRL50)
write.csv(SV, "./CombinedSoilVegData.csv", row.names = FALSE)
cors <- cor(y = SV[,c(9,13:19)],x = SV[,c(20:45)], use = 'complete.obs') #For some reason I get different correlations when I use the matrix vs. vector notation, so I have to calculate correlation for each variable individually.
#write.csv(cors, "./correlations.csv")
cor(y=SV$IRLDepth, x=SV$ARTR2_Total, use = "complete.obs")    #
cors <- cor(y = SV[,c(9,13:19)],x = SV[,c(20:45)], use = 'complete.obs') #For some reason I get different correlations when I use the matrix vs. vector notation, so I have to calculate correlation for each variable individually.
cor(y=SV$IRLDepth, x=SV$ARTR2_Total, use = "complete.obs")    #
soilsSum<-read.csv("F:/Soils/SoilEnvironmentalData.csv", header=T)
soilsOrg<-read.csv("C:/Users/Grant/Documents/Beef Basin 2013/SoilDataInputR.csv", header=T)
Sage<-read.csv("F:/ShrubDensity/SagePresenceAbsence.csv", header=T)
Live<-read.csv("F:/ShrubDensity/SageLivePresenceAbsence.csv")
Dead<-read.csv("F:/ShrubDensity/SageDeadPresenceAbsence.csv")
sum1<-soilsSum[soilsSum$id!='IL2_9',]
sumSage<- cbind(sum1,Sage)
g<-glm(as.factor(ARTR2)~maxDepth+minpH+BioticCrustClass+as.factor(CarbonateStage),data=sumSage, family= 'binomial')
summary(g)
h<-glm(as.factor(ARTR2.D)~maxDepth+minpH+minClay+maxpH+as.factor(CarbonateStage),data=sumSage, family= 'binomial')
summary(h)
plot(g)
boxplot(maxDepth~ARTR2,data = sumSage, main= 'maxDepth')
boxplot(minSand~ARTR2,data = sumSage, main= 'minSand')
boxplot(maxSand~ARTR2,data = sumSage, main= 'maxSand')
boxplot(minClay~ARTR2,data = sumSage, main= 'minClay')
boxplot(maxClay~ARTR2,data = sumSage, main= 'maxClay')
boxplot(BioticCrustClass~ARTR2,data = sumSage, main= 'BioticCrustClass')
boxplot(CarbonateStage~ARTR2,data = sumSage, main= 'CarbonateStage')
boxplot(maxpH~ARTR2,data = sumSage, main= 'maxpH')
boxplot(minpH~ARTR2,data = sumSage, main= 'minpH')
library(plyr)
#read in shrub density data and omit date, site, etc...
shrub <- read.csv('F:/ShrubDensity/ShrubSummary.csv')
library(plyr)
#read in shrub density data and omit date, site, etc...
shrub <- read.csv('F:/ShrubDensity/ShrubSummary.csv')
View(shrub)
shrub <- shrub[1:7456,c(2,3,5,6)]
View(shrub)
shrub
#Summed shrub totals across transects 2,3, and 4
#Put into Plot by Species matrix
plotXspp<-xtabs(Total~Plot+Species, shrub)
plotXspp
write.csv(plotXspp,file="F:/ShrubDensity/plotXspp.csv")
########## Shrub Density in m2 and ha ##################
#know area sampled: 30m transects X 2m belt X 3 transects per plot (2,3,&4) = 180m2
# density in m2
densityM2 <- plotXspp/180
densityM2
write.csv(densityM2,file="F:/ShrubDensity/densityM2.csv")
#density in ha
# ha = m2/10,000
densityHa <- densityM2/10000
densityHa
write.csv(densityHa,file="F:/ShrubDensity/densityHa.csv")
########### Presence/Absence ######################
#Turn shurb totals into presence/absence (1/0)
pa <- (plotXspp>0)     # logical, or
pa <- (plotXspp>0)*1L  # integer 01
pa
write.csv(pa,file="F:/ShrubDensity/ShrubPresenceAbsence.csv")
#Select only ARTR and ARTR/D presence/absence (1/0)
Sage <- pa[,3:4]
write.csv(Sage,file="F:/ShrubDensity/SagePresenceAbsence.csv")
#Select only ARTR presence/absence (1/0)
SageLive <-pa[,3]
write.csv(SageLive,file="F:/ShrubDensity/SageLivePresenceAbsence.csv")
#Select only ARTR/D presence/absence (1/0)
SageDead <-pa[,4]
write.csv(SageDead,file="F:/ShrubDensity/SageDeadPresenceAbsence.csv")
Class <- (Sage[,1] > 0)*1L ## Live Sage present
Class <- (Sage[,2] > 0)*2L ## No Live Sage present, only dead
nothing <- (Sage [,1])+ (Sage [,2])
Class <- c(((Sage[,1] > 0)*1L),((Sage[,2] > 0)*2L),((nothing < 1)*3L))  ## No Sage live or dead present
write.csv(Class,file="F:/ShrubDensity/SagebrushClasses.csv")
Class
Class <- (Sage[,1] > 0)*1L ## Live Sage present
Class <- (Sage[,2] > 0)*2L ## No Live Sage present, only dead
nothing <- (Sage [,1])+ (Sage [,2])
nothing
library(vegan)
library(MASS)
#---NMDS Script for Plant Community Ecology--------
#### Took the file F:/LPI/LPIRelativeCover.csv and used
#### SUM and COUNTIF in Excel to find the colum sums and
#### number of sites each veg was found at then I manually
#### removed those with COUNTIF below 5 AND SUMS below .05 (had to both be below to be removed)
#### Also cleaned up the data (combined SALS0 and SALSO, etc...)
data<-read.csv("F:/LPI/LPIRelativeCoverCommonInExcel.csv",header=TRUE,row.names=1)
data
write.csv(data,file="F:/LPI/data1.csv")
###remove empty columns
#lpi.full<-as.matrix(data1[-which(colSums(data1)==0),])
#lpi.full
#remove negative values
#data1[42,63]<-0
#data1[62,41]<-0
#data1
#calculate dissimilarities, use function "vegdist"in VEGAN package
data.dis<-vegdist(data,method="bray")
data.dis
dis.matrix<-as.matrix(data.dis)
dis.matrix
write.csv(dis.matrix,file="F:/LPI/LPIDistanceMatrix.csv")
#calculate isoMDS function, use isoDS function in MASS package
data.mds<-isoMDS(data.dis)
data.mds
#--------stressplot and ordiplot----------
stressplot(data.mds,data.dis)
ordiplot(data.mds,type="t")
gof = goodness(data.mds)
